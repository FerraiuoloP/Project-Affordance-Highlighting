{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fVWoAqhmIZz",
        "outputId": "e8c1a812-7673-48e1-9d98-4d31bda6184d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "#get torch version\n",
        "print(torch.__version__)\n",
        "#check if CUDA is available\n",
        "print(torch.cuda.is_available())\n",
        "#get cuda version\n",
        "print(torch.version.cuda)\n",
        "\n",
        "#define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Utils Functions\n",
        "Utility functions to help with the implementation of the solution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxcXDQmflHnz",
        "outputId": "1e03aebb-8776-4b06-d350-6f93c002d67d"
      },
      "outputs": [],
      "source": [
        "import open3d as o3d\n",
        "import numpy as np\n",
        "\n",
        "#Takes a screenshot of the point cloud and saves it to the specified path\n",
        "def render_point_cloud_to_image(pcd, image_path=\"output/tmp_screen.png\"):\n",
        "    vis = o3d.visualization.Visualizer()\n",
        "    vis.create_window(visible=False) \n",
        "    vis.add_geometry(pcd)\n",
        "    vis.poll_events()\n",
        "    vis.update_renderer()\n",
        "    vis.capture_screen_image(image_path)\n",
        "    vis.destroy_window()\n",
        "\n",
        "\n",
        "#Shows the point cloud in a window or saves it to the specified path\n",
        "def show_point_cloud(point_cloud,render_to_image=False,save_path=\"output/tmp_screen.png\"):\n",
        "    pcd = o3d.geometry.PointCloud()\n",
        "    pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
        "    if(render_to_image):\n",
        "        render_point_cloud_to_image(pcd,save_path)\n",
        "    else:\n",
        "        o3d.visualization.draw_geometries([pcd])\n",
        "    \n",
        "#Shows the point cloud in a window or saves it to the specified path, coloring the points based on the probabilities\n",
        "def show_point_cloud_tresholded(point_cloud,probs,treshold,render_to_image=False,save_path=\"output/tmp_screen.png\"):\n",
        "    \n",
        "    #if probs has 2 columns, delete the second one (refer to the \"no object\" class)\n",
        "    if probs.shape[1]==2:\n",
        "        probs = probs[:,0]\n",
        "\n",
        "    pcd = o3d.geometry.PointCloud()\n",
        "    pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
        "    colors = np.zeros((len(point_cloud),3))\n",
        "    for i in range(len(point_cloud)):\n",
        "        if probs[i]>treshold:\n",
        "            colors[i] = [1,0,0]\n",
        "        else:\n",
        "            colors[i] = [0.5,0.5,0.5]\n",
        "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
        "    if render_to_image:\n",
        "        render_point_cloud_to_image(pcd,save_path)\n",
        "    else:\n",
        "        o3d.visualization.draw([pcd])\n",
        "\n",
        "     \n",
        "def create_point_cloud_from_mesh(mesh_path,name):\n",
        "    mesh = o3d.io.read_triangle_mesh(mesh_path)\n",
        "    pcd = mesh.sample_points_uniformly(number_of_points=10000) #Tune if needed.\n",
        "    o3d.io.write_point_cloud(f\"output/{name}.ply\", pcd)\n",
        "    return pcd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load dataset\n",
        "Extract the dataset from the input file, parse it and return the relevant information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "def load_dataset(path):\n",
        "    dataset = []\n",
        "    with open(path, 'rb') as f:\n",
        "        train_data = pickle.load(f)\n",
        "        print(\"Loaded train_data\")\n",
        "        # print train_data\n",
        "        for index,info in enumerate(train_data):\n",
        "            \n",
        "            temp_info = {}\n",
        "            temp_info[\"shape_id\"] = info[\"shape_id\"]\n",
        "            temp_info[\"semantic class\"] = info[\"semantic class\"]\n",
        "            temp_info[\"affordance\"] = info[\"affordance\"]\n",
        "            temp_info[\"data_info\"] = info[\"full_shape\"]\n",
        "            dataset.append(temp_info)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functions for 3rd part\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import open3d as o3d\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Function to align the probabilities of the predicted points to the groundtruth points \n",
        "# (considering for each groundtruth point the closest point in the predicted points)\n",
        "def align_prob(gt_points,approx_mesh_points,pred_class):\n",
        "    selected_points = []\n",
        "\n",
        "    # Itero sui punti di L1\n",
        "    map_groundtruth_mesh = []\n",
        "\n",
        "    approx_mesh_points_cpy = approx_mesh_points.cpu().detach().numpy()\n",
        "\n",
        "    #Map the groundtruth points to the mesh points. It is downscaling the approx_mesh_points to match the size of the groundtruth points\n",
        "    # map_groundtruth_mesh will contain the index of the groundtruth point and the index of the mesh point\n",
        "    # selected_points will contain the mesh points thare chosen to be the closest to the groundtruth points\n",
        "    for i,point in enumerate(gt_points):\n",
        "        point_array = np.array(point)\n",
        "        \n",
        "        # Get distances from all the points in L2\n",
        "        distances = np.linalg.norm(approx_mesh_points_cpy - point_array, axis=1)\n",
        "        \n",
        "        # Get the index of the closest point\n",
        "        closest_index = np.argmin(distances)\n",
        "    \n",
        "        # Add the closest point to the selected points\n",
        "        selected_points.append(tuple(approx_mesh_points_cpy[closest_index]))\n",
        "\n",
        "        # add to map_groundtruth_mesh the tuple of  i and closest_index\n",
        "        map_groundtruth_mesh.append((i,closest_index))\n",
        "    \n",
        "        #set to inf the closest point to avoid selecting it again\n",
        "        approx_mesh_points_cpy[closest_index] = np.array([np.inf,np.inf,np.inf])\n",
        "\n",
        "\n",
        "    #pred_classes contains the prediction of the model (to color that point) for each one of the selected points\n",
        "    pred_classes_aligned = np.array([pred_class[j].detach().cpu() for _, j in map_groundtruth_mesh])\n",
        "\n",
        "    return pred_classes_aligned\n",
        "\n",
        "#average Intersection over Union: for each threshold from 0.01 to 1, calculate the IoU and average them\n",
        "def aIoU(gt_prob,pred_classes_aligned):\n",
        "\n",
        "    thresh = 0.01\n",
        "    iou=0\n",
        "    cont=0\n",
        " \n",
        "    while thresh<1:\n",
        "        intersection = len([i for i in range(len(gt_prob)) if gt_prob[i] > thresh and pred_classes_aligned[i][0] > thresh]) #both pred and gt are above threshold\n",
        "        union= len([i for i in range(len(gt_prob))  if (gt_prob[i]>thresh  or pred_classes_aligned[i][0] > thresh  )]) #either pred or gt are above threshold\n",
        "        if union==0:\n",
        "            iou+=1\n",
        "        else:\n",
        "            iou+= intersection / union\n",
        "        thresh+=0.01\n",
        "        cont+=1\n",
        "\n",
        "    aiou = iou/cont\n",
        "\n",
        "    return aiou\n",
        "\n",
        "#Mean Intersection over Union: for each threshold from 0.01 to 1, calculate the IoU and return the maximum\n",
        "def mIoU(gt_prob,pred_classes_aligned,treshold_gt,treshold_pred):\n",
        "    intersection = len([i for i in range(len(gt_prob)) if gt_prob[i] > treshold_gt and pred_classes_aligned[i][0] > treshold_pred]) #both pred and gt are above threshold\n",
        "    union= len([i for i in range(len(gt_prob))  if (gt_prob[i]>treshold_gt  or pred_classes_aligned[i][0] > treshold_pred  )]) #either pred or gt are above threshold\n",
        "    if union==0:\n",
        "        return 1\n",
        "    else:\n",
        "        return intersection / union\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functions for 2nd part\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import kaolin\n",
        "import trimesh\n",
        "import trimesh.convex  \n",
        "import open3d as o3d\n",
        "\n",
        "def create_mesh(point_cloud,mesh_path,smooth=True):\n",
        "    \n",
        "    point_cloud=torch.tensor(point_cloud).cpu()\n",
        "    min_coords, _ = point_cloud.min(dim=0)\n",
        "    max_coords, _ = point_cloud.max(dim=0)\n",
        "    original_scale = max_coords - min_coords\n",
        "    original_translation = min_coords\n",
        "\n",
        "    # Normalize the point cloud to [0, 1] range \n",
        "    normalized_point_cloud = (point_cloud - original_translation) / original_scale\n",
        "\n",
        "    resolution = 20\n",
        "    went_under=False\n",
        "\n",
        "    # Searching for the best resolution that yields a greater number of vertices than the original point cloud while minimizing total vertices\n",
        "    # Resolution = 20 chosen empirically as a good starting point\n",
        "\n",
        "    while True:\n",
        "        voxel_grid = kaolin.ops.conversions.pointclouds_to_voxelgrids(\n",
        "            normalized_point_cloud.unsqueeze(0), resolution=resolution\n",
        "        ).cuda()\n",
        "\n",
        "        # Convert voxel grid to triangle mesh\n",
        "        triangle_mesh = kaolin.ops.conversions.voxelgrids_to_trianglemeshes(\n",
        "            voxel_grid, iso_value=0.95\n",
        "        )\n",
        "        if len(triangle_mesh[0][0]) <  len(point_cloud):\n",
        "            went_under=True\n",
        "            resolution+=1\n",
        "            \n",
        "            continue\n",
        "        elif( went_under and len(triangle_mesh[0][0]) >=  len(point_cloud)):\n",
        "            print(\"Choosen res\",resolution)\n",
        "            break\n",
        "        \n",
        "        resolution-=1\n",
        "\n",
        "\n",
        "    # Extract vertices and faces from the triangle mesh\n",
        "    verts, faces = triangle_mesh\n",
        "    verts = verts[0].cpu()  \n",
        "    faces = faces[0].cpu() \n",
        "\n",
        "   \n",
        "    verts = verts / resolution  # Normalize vertices \n",
        "\n",
        "    #De-normalize vertices back to the original point cloud coordinates\n",
        "    verts = verts * original_scale + original_translation\n",
        "\n",
        "    # Create a Trimesh object\n",
        "\n",
        "\n",
        "    #compue normals\n",
        "    \n",
        "\n",
        "    if verts.numel() == 0 or faces.numel() == 0:\n",
        "        raise ValueError(\"Vertices or faces are empty. Cannot create a mesh.\")\n",
        "\n",
        "  \n",
        "\n",
        "    mesh = trimesh.Trimesh(vertices=verts.cpu().numpy(), faces=faces.cpu().numpy())\n",
        "    \n",
        "    # Fix alignment issue\n",
        "    verts[:, 1] -= 0.04\n",
        "    verts[:, 0] -= 0.01\n",
        "    verts[:,2]-=0.017\n",
        "\n",
        "    # Smooth the mesh\n",
        "    if smooth:\n",
        "        mesh = trimesh.smoothing.filter_laplacian(mesh, lamb=0.2, iterations=8, \n",
        "                                    implicit_time_integration=False, \n",
        "                                    volume_constraint=True, \n",
        "                                    laplacian_operator=None)\n",
        "\n",
        "\n",
        "    # Export to OBJ file\n",
        "    mesh.export(mesh_path)\n",
        "  \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positional encoding extension\n",
        "Load the image here, then it will be used later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load an image from file\n",
        "from PIL import Image\n",
        "\n",
        "def load_image(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    image = image.convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "img_bg=load_image(\"bg/wood.jpg\") \n",
        "\n",
        "img_bg=None  #NOTE: comment this line to use the background image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positional encoding extension\n",
        "This function will then be used in the model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def positional_encoding(x, num_freq):\n",
        "\n",
        "    frequencies = 2 ** torch.arange(num_freq, dtype=torch.float32) .to(device) # Frequencies 2^i\n",
        "    frequencies = frequencies[None, :].to(device)  # Add a batch dimension\n",
        "    x_expanded = x.unsqueeze(-1)  # Expand input dimension for broadcasting\n",
        "\n",
        "    # Compute sine and cosine embeddings\n",
        "    sin_enc = torch.sin(2 * np.pi * frequencies * x_expanded)\n",
        "    cos_enc = torch.cos(2 * np.pi * frequencies * x_expanded)\n",
        "\n",
        "    # Concatenate embeddings\n",
        "    encoding = torch.cat([sin_enc, cos_enc], dim=-1)\n",
        "    return encoding.view(*x.shape[:-1], -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "jzUD2R0NlHn0"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import copy\n",
        "import json\n",
        "import kaolin as kal\n",
        "import kaolin.ops.mesh\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from itertools import permutations, product\n",
        "from Normalization.MeshNormalizer import MeshNormalizer\n",
        "from mesh import Mesh\n",
        "from pathlib import Path\n",
        "from render import Renderer\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from utils import device, color_mesh\n",
        "\n",
        "\n",
        "class NeuralHighlighter(nn.Module):\n",
        "    def __init__(self, depth=5, width=256, out_dim=2,input_dim=3,use_pos_enc=False, pos_enc_dim=20):\n",
        "        super(NeuralHighlighter, self).__init__()\n",
        "        self.depth = depth\n",
        "        self.width = width\n",
        "        self.out_dim = out_dim\n",
        "        self.pos_enc_dim = pos_enc_dim\n",
        "        self.use_pos_enc = use_pos_enc\n",
        "      \n",
        "        if use_pos_enc: #Positional encoding extension\n",
        "            self.encoded_dim = input_dim * 2 * pos_enc_dim  \n",
        "        else:\n",
        "            self.encoded_dim = input_dim\n",
        "        # Core model\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(self.encoded_dim, width),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(width),\n",
        "        )\n",
        "\n",
        "        # Replicate the core model depth times\n",
        "        for _ in range(depth - 1):\n",
        "            self.model.append(nn.Linear(width, width))\n",
        "            self.model.append(nn.ReLU())\n",
        "            self.model.append(nn.LayerNorm(width))\n",
        "\n",
        "        # Final layers\n",
        "        self.model.append(nn.Linear(width, out_dim))\n",
        "        self.model.append(nn.Softmax(dim=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_pos_enc:\n",
        "            x_encoded = positional_encoding(x, self.pos_enc_dim) #Positional encoding extension\n",
        "            x_encoded = x_encoded.view(x.shape[0], -1)\n",
        "            return self.model(x_encoded)\n",
        "        else:\n",
        "            return self.model(x)\n",
        "\n",
        "def get_clip_model(clipmodel):\n",
        "    model, preprocess = clip.load(clipmodel)\n",
        "    return model, preprocess\n",
        "\n",
        "# ================== HELPER FUNCTIONS =============================\n",
        "def save_final_results(log_dir, mesh, mlp, vertices, colors, render, background,ply_path=None):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        # for renders\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([255,255,0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
        "                                                                        show=False,\n",
        "                                                                        center_azim=0,\n",
        "                                                                        center_elev=0,\n",
        "                                                                        std=1,\n",
        "                                                                        return_views=True,\n",
        "                                                                        lighting=True,\n",
        "                                                                        background=background)\n",
        "        # for mesh\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        mesh.export(os.path.join(log_dir, f\"{ply_path}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
        "        return rendered_images\n",
        "\n",
        "\n",
        "def clip_loss(embedding,images,clip_model,augmentations,augmentation_number):\n",
        "    loss = 0.0\n",
        "    encoded_text = clip_model.encode_text(embedding)\n",
        "    for _ in range(augmentation_number):\n",
        "        aug_img = augmentations(images)\n",
        "        encoded_imgs = clip_model.encode_image(aug_img)\n",
        "        loss -= torch.mean(torch.cosine_similarity(encoded_imgs, encoded_text))\n",
        "\n",
        "    return loss/augmentation_number\n",
        "    \n",
        "\n",
        "\n",
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizer and settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj3j3cDhlHn0",
        "outputId": "91d7e3ea-c312-4d01-e78c-dbe483861a45"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "seed = 1\n",
        "# Constrain most sources of randomness\n",
        "# (some torch backwards functions within CLIP are non-determinstic)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "render_res = 224\n",
        "learning_rate = 0.0008\n",
        "n_iter = 1500\n",
        "res = 224 \n",
        "obj_path = 'output_mesh.obj'\n",
        "n_augs = 3\n",
        "output_dir = './output/'\n",
        "# clip_version = 'ViT-L/14'\n",
        "# clip_version = 'RN50x4'\n",
        "# clip_version = 'RN50x16'\n",
        "# clip_version = 'RN50x16'\n",
        "clip_version = 'ViT-B/32'\n",
        "render_color = \"red\" # or \"yellow\".\n",
        "n_views = 5\n",
        "\n",
        "Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
        "render = Renderer(dim=(render_res, render_res))\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"data_bench/full_shape_train_data.pkl\")\n",
        "\n",
        "\n",
        "# Initialize variables\n",
        "bg = torch.tensor((.5, .5, .5)).to(device)\n",
        "log_dir = output_dir\n",
        "\n",
        "\n",
        "# list of possible colors\n",
        "rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "\n",
        "\n",
        "if(render_color==\"red\"):\n",
        "    # full_colors = [[204/255, 0., 0.], [180/255, 180/255, 180/255]]\n",
        "    full_colors = [[204/255, 0., 0.], [180/255, 180/255, 180/255]]\n",
        "elif(render_color==\"yellow\"):\n",
        "    full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "\n",
        "colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "\n",
        "# --- Prompt ---\n",
        "clip_model,preprocess = get_clip_model(clip_version)\n",
        "losses = []\n",
        "\n",
        "#normalizer for image of clip\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],std=[0.26862954, 0.26130258, 0.27577711]) #from https://github.com/openai/CLIP/issues/20\n",
        "\n",
        "\n",
        "#Add augmentation\n",
        "augmentations = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(render_res, scale=(0.5, 1.0)),\n",
        "    transforms.RandomPerspective(p=0.5,distortion_scale=0.5,fill=1),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "def optimize(vertices, mlp, tokenized_text,optim,mesh,prompt,ply_path=\"3d-render\"):\n",
        "# Optimization loop\n",
        "    for i in tqdm(range(n_iter)):\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # predict highlight probabilities\n",
        "        pred_class = mlp(vertices)\n",
        "\n",
        "        # color and render mesh\n",
        "        sampled_mesh = mesh\n",
        "        color_mesh(pred_class, sampled_mesh, colors)\n",
        "        rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
        "                                                                show=False,\n",
        "                                                                center_azim=0,\n",
        "                                                                center_elev=0,\n",
        "                                                                std=1,\n",
        "                                                                return_views=True,\n",
        "                                                                lighting=True,\n",
        "                                                                background=bg,\n",
        "                                                                bg_image=img_bg)\n",
        "                                                                # bg_img=bg_img)\n",
        "        \n",
        "        # Calculate CLIP Loss\n",
        "        loss = clip_loss(tokenized_text,rendered_images,clip_model,augmentations,n_augs)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        # update variables + record loss\n",
        "        with torch.no_grad():\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        # report results\n",
        "        if i % 100 == 0:\n",
        "            print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "            save_renders(log_dir, i, rendered_images)\n",
        "            with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n",
        "                f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "    # save results\n",
        "    final_image=save_final_results(log_dir,mesh, mlp, vertices, colors, render, bg,ply_path=ply_path)\n",
        "  \n",
        "    del optim, mesh, rendered_images\n",
        "    torch.cuda.empty_cache()\n",
        "    losses.clear()\n",
        "    return pred_class,final_image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_test_val(choosen_affordance,choosen_prompt,obj_class,test_val_dataset,gt_treshold=0.05, pred_treshold=0.3):\n",
        "    iterations=0\n",
        "    aiou=0\n",
        "    miou=0\n",
        "\n",
        "    for obj_num in range(test_val_dataset[obj_class][0],test_val_dataset[obj_class][1]):\n",
        "            img=[None,None,None]\n",
        "            iterations+=1\n",
        "            mesh_path=f\"output/{obj_class}_test_val_tmp.obj\"\n",
        "           \n",
        "            create_mesh(dataset[obj_num][\"data_info\"][\"coordinate\"],mesh_path,smooth=True) #Set smooth to False if  preferred\n",
        "          \n",
        "            mesh = Mesh(obj_path=mesh_path)\n",
        "            MeshNormalizer(mesh)()\n",
        "            mlp = NeuralHighlighter().to(device)\n",
        "            optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
        "            prompt = choosen_prompt.format(obj_name=obj_class,color=render_color)\n",
        "            \n",
        "            print(\"Prompt:\",prompt)\n",
        "            tokenized_text = clip.tokenize([prompt]).to(device) \n",
        "            vertices = copy.deepcopy(mesh.vertices)\n",
        "           \n",
        "            pred_class,final_render=optimize(vertices, mlp, tokenized_text,optim,mesh,obj_class)\n",
        "            pred_class_aligned=align_prob(dataset[obj_num][\"data_info\"][\"coordinate\"],mesh.vertices,pred_class)\n",
        "            #show in the notebook the final render\n",
        "            img[0] = final_render[0].cpu().detach().numpy().transpose(1, 2, 0)\n",
        "           \n",
        "            show_point_cloud_tresholded(dataset[obj_num][\"data_info\"][\"coordinate\"], pred_class_aligned, pred_treshold,render_to_image=True,save_path=\"output/tmp_screen.png\")\n",
        "            img[1] = plt.imread(\"output/tmp_screen.png\")\n",
        "\n",
        "\n",
        "            show_point_cloud_tresholded(dataset[obj_num][\"data_info\"][\"coordinate\"], dataset[obj_num][\"data_info\"][\"label\"][choosen_affordance],gt_treshold,render_to_image=True,save_path=\"output/tmp_screen.png\")\n",
        "            img[2] = plt.imread(\"output/tmp_screen.png\")\n",
        "\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))  \n",
        "            for i, ax in enumerate(axes):\n",
        "                ax.imshow(img[i])\n",
        "                ax.axis(\"off\")\n",
        "            axes[0].set_title(\"Final Mesh\")\n",
        "            axes[1].set_title(\"Predicted\")\n",
        "            axes[2].set_title(\"Ground Truth\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "            aiou_tmp=aIoU(dataset[obj_num][\"data_info\"][\"label\"][choosen_affordance],pred_class_aligned)\n",
        "            mIoU_tmp=mIoU(dataset[obj_num][\"data_info\"][\"label\"][choosen_affordance],pred_class_aligned,gt_treshold,pred_treshold)\n",
        "            print(f\"{obj_class} number {obj_num}: [aIoU: {aiou_tmp}, mIoU:{mIoU_tmp} ]\")\n",
        "            aiou+=aiou_tmp\n",
        "            miou+=mIoU_tmp\n",
        "    return aiou/iterations,miou/iterations\n",
        "\n",
        "\n",
        "def run_test_single_obj(choosen_prompt,object_name,mesh=None,ply_path=\"3d-render\"):\n",
        "    if mesh is None:\n",
        "        mesh_path=f\"data/{object_name}.obj\"\n",
        "        mesh = Mesh(obj_path=mesh_path)\n",
        "        MeshNormalizer(mesh)()\n",
        "    mlp = NeuralHighlighter().to(device)\n",
        "    optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
        "    prompt = choosen_prompt.format(obj_name=object_name,color=render_color)\n",
        "    print(\"Prompt:\",prompt)\n",
        "    tokenized_text = clip.tokenize([prompt]).to(device)\n",
        "    vertices = copy.deepcopy(mesh.vertices)\n",
        "    pred_class=optimize(vertices, mlp, tokenized_text,optim,mesh,object_name,ply_path)\n",
        "    \n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Third part test-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Define the dataset\n",
        "allowed_affordance={\"door\":[\"openable\",\"pull\"],\"vase\":[\"pourable\"],\"sink\":[\"openable\"],\"chair\":[\"sittable\"],\"earphone\":[\"grasp\"],\"scissors\":[\"cut\"],\"bed\":[\"layable\"],\"bottle\":[\"openable\"]}\n",
        "invalid_list=[] \n",
        "#Define the test/val set for each object (the range of the indexes in the dataset)\n",
        "obj_validation={\"earphone\":[639,660],\"bed\":[15283,15300],\"chair\":[3088,3090],\"bottle\":[7368,7383]}  \n",
        "obj_test={\"earphone\":[660,700],\"bed\":[15300,15340],\"bottle\":[7410,7420]}\n",
        "\n",
        "\n",
        "\n",
        "#Define test case (change this)\n",
        "choosen_affordance=\"layable\"\n",
        "choosen_class=\"earphone\"\n",
        "choosen_prompt=\"A 3D render of a gray \" +choosen_class+\" with the graspable surface colored in {color}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Run validation\n",
        "aiou,miou=run_test_val(choosen_affordance,choosen_prompt,choosen_class,obj_validation)\n",
        "print(f\"Validation final results: [aIoU: {aiou}, mIoU:{miou} ]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Run test\n",
        "aiou,miou=run_test_val(choosen_affordance,choosen_prompt,choosen_class,obj_test)\n",
        "print(f\"Test final results: [aIoU: {aiou}, mIoU:{miou}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Single run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "choosen_prompt=\"A 3D render of a gray {obj_name} with the poncho in red\"\n",
        "run_test_single_obj(choosen_prompt,object_name=\"horse\",mesh=None,ply_path=\"3d-render\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extension multi class highligting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### DBSCAN to cluster the points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import open3d as o3d\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "\n",
        "\n",
        "# Use RANSAC to find geometric planes (or other models, e.g., spheres)\n",
        "def ransac_segmentation(pcd):\n",
        "\n",
        "    # Fit planes using RANSAC\n",
        "    plane_model, inliers = pcd.segment_plane(distance_threshold=0.02, ransac_n=3, num_iterations=10000)\n",
        "    \n",
        "    # Extract the points that belong to the plane\n",
        "    inlier_points = pcd.select_by_index(inliers)\n",
        "    outlier_points = pcd.select_by_index(inliers, invert=True)\n",
        "    \n",
        "    return inlier_points, outlier_points\n",
        "\n",
        "# Use DBSCAN for unsupervised clustering of the remaining outlier points (e.g. dog and horse)\n",
        "def dbscan_clustering(points):\n",
        "    points = np.asarray(points.points)  # Convert Open3D point cloud to numpy array\n",
        "    \n",
        "    # Perform DBSCAN clustering\n",
        "    db = DBSCAN(eps=0.2, min_samples=10)  # Adjust eps \n",
        "    labels = db.fit_predict(points)\n",
        "    \n",
        "    return labels\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Multi-class highlighting loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import open3d as o3d\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import gc\n",
        "import cv2\n",
        "\n",
        "\n",
        "augmentations2 = transforms.Compose([\n",
        "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],std=[0.26862954, 0.26130258, 0.27577711])\n",
        "])\n",
        "\n",
        "candiate_labels=[\"dog\",\"horse\",\"scissors\",\"knife\",\"table\",\"chair\",\"airplane\"] #possible labels that every object can be classified as\n",
        "\n",
        "\n",
        "def color_point_cloud_grey(pcd):\n",
        "    num_points = np.asarray(pcd.points).shape[0]\n",
        "    grey_color = np.full((num_points, 3), 0.5)  # Grey color (RGB)\n",
        "    pcd.colors = o3d.utility.Vector3dVector(grey_color)\n",
        "    return pcd\n",
        "# Function to render and classify the object\n",
        "def render_and_classify(pcd):\n",
        "    model2,preprocess2 = get_clip_model(\"ViT-L/14\")\n",
        "    model2.eval()  # Set the model to evaluation mode\n",
        "    render2 = Renderer(dim=(render_res, render_res))\n",
        "    pcd= color_point_cloud_grey(pcd)\n",
        "    #create mesh\n",
        "    bg = torch.tensor((1., 1., 1.)).to(\"cuda\")\n",
        "    highlight2 = torch.tensor([100,100,100]).to(device)\n",
        "    create_mesh(np.asarray(pcd.points),\"output/temp.obj\",smooth=False)\n",
        "\n",
        "    #load mesh\n",
        "    mesh=Mesh(obj_path=\"output/temp.obj\")\n",
        "    MeshNormalizer(mesh)()\n",
        "\n",
        "    #create a tensor of shape len(mesh.vertices),2 where the first column is 1 and the second is 0\n",
        "    pred_class= torch.ones(len(mesh.vertices),2).to(device)\n",
        "    pred_class[:,1]=0\n",
        "\n",
        "    color_mesh(pred_class,mesh,colors)\n",
        "    \n",
        "    rendered_image, elev, azim = render2.render_views(mesh, num_views=10,\n",
        "                                                                show=False,\n",
        "                                                                center_azim=0,\n",
        "                                                                center_elev=0,\n",
        "                                                                std=1,\n",
        "                                                                return_views=True,\n",
        "                                                                lighting=True,\n",
        "                                                                background=bg)\n",
        "\n",
        "    # Calculate CLIP loss for each label\n",
        "    losses2 = []\n",
        "    for label_embedding in candiate_labels:\n",
        "        label_loss = clip_loss(\n",
        "                embedding= clip.tokenize([f\"A 3D render of a {render_color} {label_embedding}\"]).to(device),\n",
        "                images=rendered_image,\n",
        "                clip_model=model2,\n",
        "                augmentations=augmentations2,\n",
        "                augmentation_number=1  # Number of augmentations\n",
        "            )\n",
        "        losses2.append(label_loss.item())\n",
        "\n",
        "    # Find the label with the lowest loss\n",
        "    \n",
        "    predicted_label = candiate_labels[np.argmin(losses2)]\n",
        "    del model2  \n",
        "    torch.cuda.empty_cache()  \n",
        "    gc.collect()  \n",
        "    return mesh,predicted_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Settings for the multi-class highlighting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "point_clouds_path = \"data/chair_table.ply\"\n",
        "mch_prompt=\"A 3D render of a gray {obj_name} with colored in {color} the sittable part\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run multi-class highlighting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "pdc = o3d.io.read_point_cloud(point_clouds_path)\n",
        "\n",
        "# multi-order RANSAC\n",
        "inlier_points, outlier_points = ransac_segmentation(pdc)\n",
        "\n",
        "# Perform DBSCAN clustering on the outlier points\n",
        "labels = dbscan_clustering(outlier_points)\n",
        "\n",
        "# Visualize the clusters (separate objects)\n",
        "unique_labels = set(labels)\n",
        "clustered_points = []\n",
        "\n",
        "for label in unique_labels:\n",
        "    if label == -1:  # Noise points, skip them\n",
        "        continue\n",
        "    cluster = outlier_points.select_by_index(np.where(labels == label)[0])\n",
        "    clustered_points.append(cluster)\n",
        "    # o3d.visualization.draw_geometries([cluster])\n",
        "\n",
        "\n",
        "\n",
        "images=[]\n",
        "\n",
        "meshes = [None] * len(clustered_points)\n",
        "labels = [None] * len(clustered_points)\n",
        "for i,cluster in enumerate(clustered_points):\n",
        "    meshes[i],labels[i] = render_and_classify(cluster)\n",
        "    \n",
        "for i,mesh in enumerate(meshes):\n",
        "    print(\"Predicted label:\", labels[i])\n",
        "    run_test_single_obj(mch_prompt,labels[i],meshes[i],ply_path=f\"3d-render_{i}\")\n",
        "    #READ THE IMAGE\n",
        "    img = cv2.imread(\"output/final_render.jpg\")\n",
        "    images.append(img)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "# Show the rendered images in the notebook\n",
        "for img in images:\n",
        "    display(Image.fromarray(img[:,:,::-1]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Code to add a custom surface to the bottom of a 3d mesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# mesh=o3d.io.read_triangle_mesh(\"data/Clip.obj\")\n",
        "# #save as point cloud\n",
        "# pcd = mesh.sample_points_uniformly(number_of_points=20000)\n",
        "\n",
        "# # Remove the points that have y=0 (the table)\n",
        "# points = np.asarray(pcd.points)\n",
        "# print(points)\n",
        "\n",
        "# #generate a table under the object\n",
        "# table = np.random.rand(100000, 3) * 0.2\n",
        "# table[:, 1] = -0.0042\n",
        "# table[:, 0] = table[:, 0] - 0.1\n",
        "# table[:, 2] = table[:, 2] - 0.1\n",
        "# points = np.append(points, table, axis=0)\n",
        "\n",
        "# points = points[(points[:, 1] >= -0.0042) & ((points[:, 0] > -0.018) & (points[:, 0] < 0.0175)) & ((points[:, 2] > -0.025) & (points[:, 2] < 0.028))]\n",
        "\n",
        "# pcd.points = o3d.utility.Vector3dVector(points)\n",
        "# print(points)\n",
        "# o3d.io.write_point_cloud(\"data/intermediate.ply\", pcd)\n",
        "\n",
        "# # convert pcd to mesh\n",
        "# create_mesh(np.asarray(pcd.points),\"data/Clip2.obj\",smooth=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3dv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
