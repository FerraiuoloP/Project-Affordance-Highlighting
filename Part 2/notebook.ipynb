{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fVWoAqhmIZz",
        "outputId": "e8c1a812-7673-48e1-9d98-4d31bda6184d"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "#get torch version\n",
        "print(torch.__version__)\n",
        "#check if CUDA is available\n",
        "print(torch.cuda.is_available())\n",
        "#get cuda version\n",
        "print(torch.version.cuda)\n",
        "\n",
        "#define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load dataset\n",
        "Dataset of the part 3 used for testing the mesh generations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "def load_dataset(path):\n",
        "    dataset = []\n",
        "    with open(path, 'rb') as f:\n",
        "        train_data = pickle.load(f)\n",
        "        print(\"Loaded train_data\")\n",
        "        # print train_data\n",
        "        for index,info in enumerate(train_data):\n",
        "            \n",
        "            temp_info = {}\n",
        "            temp_info[\"shape_id\"] = info[\"shape_id\"]\n",
        "            temp_info[\"semantic class\"] = info[\"semantic class\"]\n",
        "            temp_info[\"affordance\"] = info[\"affordance\"]\n",
        "            temp_info[\"data_info\"] = info[\"full_shape\"]\n",
        "            dataset.append(temp_info)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2 implementation - Mesh generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import kaolin\n",
        "import trimesh\n",
        "import trimesh.convex  \n",
        "\n",
        "def create_mesh(point_cloud,mesh_path,smooth=True):\n",
        "    \n",
        "    point_cloud=torch.tensor(point_cloud).cpu()\n",
        "    min_coords, _ = point_cloud.min(dim=0)\n",
        "    max_coords, _ = point_cloud.max(dim=0)\n",
        "    original_scale = max_coords - min_coords\n",
        "    original_translation = min_coords\n",
        "\n",
        "    # Normalize the point cloud to [0, 1] range \n",
        "    normalized_point_cloud = (point_cloud - original_translation) / original_scale\n",
        "\n",
        "    resolution = 20\n",
        "    went_under=False\n",
        "\n",
        "    # Searching for the best resolution that yields a greater number of vertices than the original point cloud while minimizing total vertices\n",
        "    # Resolution = 20 chosen empirically as a good starting point\n",
        "\n",
        "    while True:\n",
        "        voxel_grid = kaolin.ops.conversions.pointclouds_to_voxelgrids(\n",
        "            normalized_point_cloud.unsqueeze(0), resolution=resolution\n",
        "        ).cuda()\n",
        "\n",
        "        # Convert voxel grid to triangle mesh\n",
        "        triangle_mesh = kaolin.ops.conversions.voxelgrids_to_trianglemeshes(\n",
        "            voxel_grid, iso_value=0.95\n",
        "        )\n",
        "        if len(triangle_mesh[0][0]) <  len(point_cloud):\n",
        "            went_under=True\n",
        "            resolution+=1\n",
        "            \n",
        "            continue\n",
        "        elif( went_under and len(triangle_mesh[0][0]) >=  len(point_cloud)):\n",
        "            print(\"Choosen res\",resolution)\n",
        "            break\n",
        "        \n",
        "        resolution-=1\n",
        "\n",
        "\n",
        "    # Extract vertices and faces from the triangle mesh\n",
        "    verts, faces = triangle_mesh\n",
        "    verts = verts[0].cpu()  \n",
        "    faces = faces[0].cpu() \n",
        "\n",
        "   \n",
        "    verts = verts / resolution  # Normalize vertices \n",
        "\n",
        "    #De-normalize vertices back to the original point cloud coordinates\n",
        "    verts = verts * original_scale + original_translation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if verts.numel() == 0 or faces.numel() == 0:\n",
        "        raise ValueError(\"Vertices or faces are empty. Cannot create a mesh.\")\n",
        "\n",
        "  \n",
        "\n",
        "    mesh = trimesh.Trimesh(vertices=verts.cpu().numpy(), faces=faces.cpu().numpy())\n",
        "    \n",
        "    # Fix alignment issue\n",
        "    verts[:, 1] -= 0.04\n",
        "    verts[:, 0] -= 0.01\n",
        "    verts[:,2]-=0.017\n",
        "\n",
        "    # Smooth the mesh\n",
        "    if smooth:\n",
        "        mesh = trimesh.smoothing.filter_laplacian(mesh, lamb=0.2, iterations=8, \n",
        "                                    implicit_time_integration=False, \n",
        "                                    volume_constraint=True, \n",
        "                                    laplacian_operator=None)\n",
        "\n",
        "\n",
        "    # Export to OBJ file\n",
        "    mesh.export(mesh_path)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jzUD2R0NlHn0"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import copy\n",
        "import json\n",
        "import kaolin as kal\n",
        "import kaolin.ops.mesh\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "from itertools import permutations, product\n",
        "from Normalization.MeshNormalizer import MeshNormalizer\n",
        "from mesh import Mesh\n",
        "from pathlib import Path\n",
        "from render import Renderer\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from utils import device, color_mesh\n",
        "\n",
        "\n",
        "\n",
        "class NeuralHighlighter(nn.Module):\n",
        "    def __init__(self, depth=2, width=256, out_dim=2,input_dim=3):\n",
        "        super(NeuralHighlighter, self).__init__()\n",
        "        self.depth = depth\n",
        "        self.width = width\n",
        "        self.out_dim = out_dim\n",
        "      \n",
        "\n",
        "        # Core model\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, width),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(width),\n",
        "        )\n",
        "\n",
        "        # Replicate the core model depth times\n",
        "        for _ in range(depth - 1):\n",
        "            self.model.append(nn.Linear(width, width))\n",
        "            self.model.append(nn.ReLU())\n",
        "            self.model.append(nn.LayerNorm(width))\n",
        "\n",
        "        # Final layers\n",
        "        self.model.append(nn.Linear(width, out_dim))\n",
        "        self.model.append(nn.Softmax(dim=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        # Pass through the model\n",
        "        return self.model(x)\n",
        "\n",
        "def get_clip_model(clipmodel):\n",
        "    model, preprocess = clip.load(clipmodel)\n",
        "    return model, preprocess\n",
        "\n",
        "# ================== HELPER FUNCTIONS =============================\n",
        "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        # for renders\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
        "                                                                        show=False,\n",
        "                                                                        center_azim=0,\n",
        "                                                                        center_elev=0,\n",
        "                                                                        std=1,\n",
        "                                                                        return_views=True,\n",
        "                                                                        lighting=True,\n",
        "                                                                        background=background)\n",
        "        # for mesh\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
        "\n",
        "\n",
        "def clip_loss(embedding,images,clip_model,augmentations,augmentation_number):\n",
        "    loss = 0.0\n",
        "    encoded_text = clip_model.encode_text(embedding)\n",
        "    for _ in range(augmentation_number):\n",
        "        aug_img = augmentations(images)\n",
        "        encoded_imgs = clip_model.encode_image(aug_img)\n",
        "        loss -= torch.mean(torch.cosine_similarity(encoded_imgs, encoded_text))\n",
        "\n",
        "    return loss/augmentation_number\n",
        "    \n",
        "\n",
        "\n",
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Core loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj3j3cDhlHn0",
        "outputId": "91d7e3ea-c312-4d01-e78c-dbe483861a45"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "seed = 1\n",
        "# Constrain most sources of randomness\n",
        "# (some torch backwards functions within CLIP are non-determinstic)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "render_res = 224\n",
        "# learning_rate = 0.0001\n",
        "learning_rate = 0.0008\n",
        "n_iter = 1800 \n",
        "# obj_path = '/content/Affordance_Highlighting_Project_2024/data/horse.obj'\n",
        "n_augs = 5\n",
        "output_dir = './output/'\n",
        "# clip_version = 'ViT-L/14'\n",
        "# clip_version = 'RN50x4'\n",
        "# clip_version = 'RN50x16'\n",
        "# clip_version = 'RN50x16'\n",
        "clip_version = 'ViT-B/32'\n",
        "object_number = 1000\n",
        "object_name=\"vase\"\n",
        "mesh_path=f\"data/{object_name}.obj\"\n",
        "n_views = 5\n",
        "\n",
        "\n",
        "\n",
        "Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "objbase, extension = os.path.splitext(os.path.basename(mesh_path))\n",
        "\n",
        "render = Renderer(dim=(render_res, render_res))\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"data_bench/full_shape_train_data.pkl\")\n",
        "\n",
        "\n",
        "create_mesh(dataset[object_number][\"data_info\"][\"coordinate\"],mesh_path)\n",
        "\n",
        "mesh = Mesh(obj_path=mesh_path)\n",
        "MeshNormalizer(mesh)()\n",
        "\n",
        "\n",
        "# Initialize variables\n",
        "bg = torch.tensor((1., 1., 1.)).to(device)\n",
        "log_dir = output_dir\n",
        "\n",
        "\n",
        "\n",
        "# MLP Settings\n",
        "mlp = NeuralHighlighter().to(device)\n",
        "optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
        "\n",
        "# list of possible colors\n",
        "rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "\n",
        "colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "\n",
        "# --- Prompt ---\n",
        "# encode prompt with CLIP\n",
        "clip_model,preprocess = get_clip_model(clip_version)\n",
        "# print(model)\n",
        "prompt = 'A 3D render of a gray vase with highlighted hat'\n",
        "tokenized_text = clip.tokenize([prompt]).to(device) \n",
        "\n",
        "vertices = copy.deepcopy(mesh.vertices)\n",
        "\n",
        "\n",
        "losses = []\n",
        "\n",
        "#normalizer for image of clip\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],std=[0.26862954, 0.26130258, 0.27577711]) #from https://github.com/openai/CLIP/issues/20\n",
        "\n",
        "\n",
        "#Add augmentation\n",
        "augmentations = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(render_res, scale=(0.5, 1.0)),\n",
        "    transforms.RandomPerspective(p=0.5,distortion_scale=0.5,fill=1),\n",
        "    normalize\n",
        "])\n",
        " \n",
        "# Optimization loop\n",
        "for i in tqdm(range(n_iter)):\n",
        "    optim.zero_grad()\n",
        "\n",
        "    # predict highlight probabilities\n",
        "    pred_class = mlp(vertices)\n",
        "\n",
        "    # color and render mesh\n",
        "    sampled_mesh = mesh\n",
        "    color_mesh(pred_class, sampled_mesh, colors)\n",
        "    rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
        "                                                            show=False,\n",
        "                                                            center_azim=0,\n",
        "                                                            center_elev=0,\n",
        "                                                            std=1,\n",
        "                                                            return_views=True,\n",
        "                                                            lighting=True,\n",
        "                                                            background=bg)\n",
        "                                                            \n",
        "    # Calculate CLIP Loss\n",
        "    loss = clip_loss(tokenized_text,rendered_images,clip_model,augmentations,augmentation_number=n_augs)\n",
        "    loss.backward(retain_graph=True)\n",
        "\n",
        "    optim.step()\n",
        "\n",
        "    # update variables + record loss\n",
        "    with torch.no_grad():\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # report results\n",
        "    if i % 100 == 0:\n",
        "        print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "        save_renders(log_dir, i, rendered_images)\n",
        "        with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n",
        "            f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# save results\n",
        "save_final_results(log_dir, \"3d-render\",mesh, mlp, vertices, colors, render, bg)\n",
        "\n",
        "# Save prompts\n",
        "with open(os.path.join(output_dir, prompt), \"w\") as f:\n",
        "    f.write('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHzYDKmCAcXu"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3dv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
